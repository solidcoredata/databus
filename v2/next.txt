# Milestones

 * POCv1 - Hard code schema types, output schema, stored procs, front JSON, and back JSON.
   - Use existing syntax.
   - Hardcode types.
   - Ignore concepts like "tags".
   - Do have portals and queries.
   - Generate create schema.
   - Generate create procs.
   - Generate front end JSON that specifies menus and backend call names.
   - Generate back end JSON that contains permissions to backend calls and what DB procs they call.
 * POCv2 - Make simple backend and simple frontend to consume front and back JSON.
   - We ignore all custom middle code and features (ignore reports, special features)
   - Backend implments authentication and proc routing, and SQL marshaling.
   - Front end loads a single JSON file that configures a menu and pages.
   - Ignore workflow for now. 
 * POCv3 - Allow defining schema types in configuration.
   - This will be important for future UI types.
   - Probably useful for things like tags or table types or similar.
 * POCv4 - Create system to check-in changes record versions, and make alters.
   - Detect backwards compatible changes along with migration scripts for each step.
   - Basic commands to checkpoint changes.
 * POCv5 - Design simple system(s) to deploy alters and applications in sync with each other.
   - Rollout in order: DB changes, DB procs, backend code + configuration, Front end code + configuration, Remove Versions N-2 old.
   - Simple commands to get commands for each of these steps.
 * POCv6 - Evalutate configuration language for possible improvements.


## POCv1

 * Determine exact nature of types for this round.
 * Determine how to work with the parsed types.

---
For the following table definition:

```
context database.library

set table.book db.table{
	tags softdelete, audit
	// This columns won't have a type when initially parsing, but that is okay, the type will be known from db.table.columns type.
	// We know that this is a table/list because it uses "()" rather then "{}".
	columns ( 
		name      | type                    | default 
		---
		id        | key(int)                | serial 
		name      | text(1000)              |
		pages     | int                     |
		genre     | fk(int, table.genre.id) |
		published | bool                    | false
	)
}
```

The following list of facts are declared from the next "set" statement.
We start in an existing context, then each value has it's own fact row.
Values that have "sub-values" are naturally a "type", but they are also "values".
For instance, see "name.type = text" and "genre.type = fk".

```
database.library.table.book = db.table
database.library.table.book(db.table). // Common prefix for all of the following:
	tags[0] = softdelete
	tags[1] = audit
	columns.id.type = key
	columns.id.type(key).type[0] = int
	columns.id.default = serial
	columns.name.type = text
	columns.name.type(text).length[0] = 1000
	columns.pages.type = int
	columns.genre.type = fk
	columns.genre.type(fk).type[0] = int
	columns.genre.type(fk).reference[1] = table.genre.id
	columns.published.type = bool
	columns.published.default = false
```

Then for the following query:

```
set SimpleSelect query{
	from books b
	from genre g
	and eq(b.published, true)
	select b.id, b.bookname
	select g.name genrename
	and eq(b.deleted, false)
}
```

The "," in the select is a "repeat-verb" in the following items.
So it means this resolves to:


```
SimpleSelect = query
SimpleSelect[0] = from
SimpleSelect[0].from[0] = books
SimpleSelect[0].from[1] = b
SimpleSelect[1] = from
SimpleSelect[1].from[0] = genre
SimpleSelect[1].from[1] = g
SimpleSelect[2] = and
SimpleSelect[2].and = eq
SimpleSelect[2].and[0].eq[0] = b.published
SimpleSelect[2].and[0].eq[1] = true
SimpleSelect[3] = select
SimpleSelect[3].select[0] = b.id
SimpleSelect[4] = select
SimpleSelect[4].select[0] = b.bookname
SimpleSelect[5] = select
SimpleSelect[5].select[0] = g.name
SimpleSelect[5].select[1] = genrename
SimpleSelect[6] = and
SimpleSelect[6].and = eq
SimpleSelect[6].and[0].eq[0] = b.deleted
SimpleSelect[6].and[0].eq[1] = false
```

Now we resolve the following:

```
set SimpleInsert query{
	from books b
	and eq(b.id, 12) // Duplicate this row
	and eq(b.deleted, false)
	insert b
	set bookname b.bookname
	set pages b.pages
	set genre @genre // This is an input parameter.
	set published false
}

=>

SimpleInsert = query
SimpleInsert[0] = from
SimpleInsert[0].from[0] = books
SimpleInsert[0].from[1] = b
SimpleInsert[1] = and
SimpleInsert[1].and[0] = eq
SimpleInsert[1].and[0].eq[0] = b.id
SimpleInsert[1].and[0].eq[1] = 12
SimpleInsert[2] = and
SimpleInsert[2].and[0] = eq
SimpleInsert[2].and[0].eq[0] = b.deleted
SimpleInsert[2].and[0].eq[1] = false
SimpleInsert[3] = insert
SimpleInsert[3].insert[0] = b
SimpleInsert[4] = set
SimpleInsert[4].set[0] = bookname
SimpleInsert[4].set[1] = b.bookname
SimpleInsert[5] = set
SimpleInsert[5].set[0] = pages
SimpleInsert[5].set[1] = b.pages
SimpleInsert[6] = set
SimpleInsert[6].set[0] = genre
SimpleInsert[6].set[1] = @genre
SimpleInsert[7] = set
SimpleInsert[7].set[0] = published
SimpleInsert[7].set[1] = false

---

set SimpleUpdate query{
	from books b
	and eq(b.id, 12)
	update b
	set pages @pages
}

=>

SimpleUpdate = query
SimpleUpdate[0] = from
SimpleUpdate[0].from[0] = books
SimpleUpdate[0].from[1] = b
SimpleUpdate[1] = and
SimpleUpdate[1].and[0] = eq
SimpleUpdate[1].and[0].eq[0] = b.id
SimpleUpdate[1].and[0].eq[0] = 12
SimpleUpdate[2] = update
SimpleUpdate[2].update[0] = b
SimpleUpdate[3] = set
SimpleUpdate[3].set[0] = pages
SimpleUpdate[3].set[1] = @pages

---

set SimpleDelete query{
	from books b
	and eq(b.id, 12)
	delete b
}

=>

SimpleDelete = query
SimpleDelete[0] = from
SimpleDelete[0].from[0] = books
SimpleDelete[0].from[1] = b
SimpleDelete[1] = and
SimpleDelete[1].and[0] = eq
SimpleDelete[1].and[0].eq[0] = b.id
SimpleDelete[1].and[0].eq[0] = 12
SimpleDelete[2] = delete
SimpleDelete[2].delete[0] = b

```

---

So then lastly, we come to how to specify a schema.

```
property zero_more,number
==
property zero_more
property number
```

"number" is any type of number of any size. Is NOT a decimal or float or anything, just a number-like token.
"int64" is a type of "number", as is "decimal" and "float64".
"text" is any type of text.

property int64, text
property text

The following is what a the table schema may look like:
```
package db // module solidcoredata.com/c/db

schema table struct {
	tags arity(zero_more), value(identifier), valid(query{
		from parent.tags
		select identifer
	})
	columns arity(one_more), children(struct{
		name key
		type ...
		default zero_one
	})
}
```
---

Make types composite. Types are just values.

alias int number(decimal 0) // This is not real syntax (right now), but in backend this is what "int" should evaluate to.

set database.library.table.book table(
	name   | type
	---    | ---
	id     | key(int)
	title  | text(100) or text(max 100)
	price  | decimal(16,2) or decimal(precision 16, scale 2)
	stock  | number(min 0, max 1000000, decimal 0)
	price2 | number(min 0.01, max 10000, decimal 2)
)

// An End Of Statement (EOS) is a ";". These are inserted automatically by the lexer
// when a non-blank line does not end in a symbol.
//
// A table divides cells with a "|".
//
// A list divides up items with a ",".
//
// A list item is not the same as a table cell.

Repetition ","
EOS ";"
EOV "|"
EOT " " (any whitespace)

Or maybe "{}" is a struct type, and "()" is a "list/table" type.
Struct types expect the first value to be the type, then a "," will repeate the type and proceed with additional values.
Where as a "()" will not expect the repetition / struct fields.


// Don't worry about extending the model at this time.
// Yes, we will want to extend the model in the future.
// But there isn't anything dramatic from adding that design.

schema table schema ( 
	name | text(min: 1, max: 1000)
	type | $.type
	nullable | bool | (default false)
)


// Tools may display author table as a child of the book table.
// Furthermore, when finalizing the table, it will look down the chain
// of identifiers and look for the first "db" type for the correct database
// to put it in. Each table will then register itself with the database
// and in a global table registry, as each database will also register itself
// globally. Registration will be by types.
//
//   var Register = map[string]map[string]Struct // map[type]map[full_identifier]Struct
//   type Struct interface {
//       Type() string
//       // Span()...
//   }
//
// The context identifier, relative identifier, and full identifier need to be
// accessable in the struct for resolving relative identifiers.
//
// In a similar way, Named Filters on a table may be declared in any namespace above
// the table. Generated or tag named filters may be under "mytagname.first_name_contains".
// This filter can be referred to as "first_name_contains". If there are any named filters
// with a conflicting name such as "generated.first_name_contains", then "mytagname.first_name_contains"
// must be used or it will fail to compile.
//
// For tags use a tool to generate the additional columns and named filters.

 1. Load up every reference in it's own path. Do NOT resolve values.
 2. Determine the type of each path.
    a. It may be declared "struct(type db.table)" directly.
	b. It may be implied from a parent type:
	   "db.tables.book" looks at book (no type),
	   looks at tables (indirect type from db struct type def),
	   looks at db (type declared).
 3. Marshal values of each path into usable types.
 4. Validate references within values.
    a. For POCv1, do this with custom code.
	b. Later, it could be interesting to turn all values into tables, where each type is a unique table.
	   Then validation and type checking happen by defining queries on each type.
	   - https://godoc.org/modernc.org/sqlite
	   - https://godoc.org/modernc.org/ql
	   - https://gitlab.com/users/cznic/projects

In SQL Standard, you could look at the number of parts an ID has to determine what
the left most part was refering to. If more then one, left would be DB or schema, then table, then column.
In this, I need some way to denote Global ref (first part DB), DB ref (table), Table ref (column).

This can be done by doing some type of reader plus the most recent struct property name. For instance,
when setting the value "database.library.table.book.column.genre.type" the following could be referenced:
 * "$table.genre.column.id" would reference the table "genre.id".
 * "table$.column.other_genre" would reference the column "other_genre" on the same table (yeah, not a good example).
But there would be one syntax to go down the property before it, allowing going up to a new sibling,
where the other syntax would go down to the it's own parent.

If types such as "text(max 100)" is just a pointer to the text table where the column "max=100",
Then this could also be used to create a generic text type. Rather then pointing to an individual "text(100)",
you could point it to a generial "app1.longtext" type which has a value of "text(max 100)".
Because these are copy on assignment, changing these values after assignment will be perfectly fine.


List of types:
	db
	db.table
	db.table.tags list(identifier)
	db.table.columns table(name key; type db.type; default db.query)
	db.table.index table(name key; index list(identifier); include list(idnetifier); where query)
	db.type.key (db.type)
	db.type.fk(type db.type, reference identifier) // NOTE: I don't think fk should contain a type, but use the reference type.
	db.type.text (max uint)
	db.type.int
	db.type.uint
	db.type.float
	db.type.decimal (max uint, decimals uint)
	db.type.bool
	db.type.bytes (max uint)
	db.type.date
	db.type.datetime
	db.type.datetimez
	db.query
	atom (id identifier, parent ref nullable, rel identifier, type_name identifier, type_key key)// A "set" statment.

Each type internally creates a new table.
Initially identifiers are not resolved.

Each value in a struct has a parent ref, if lexically contained within that struct.
This should be used for printing only.
I'm strongly considering limiting where comments can be placed.
Comments may reside on a line before statement or the line before a row, or be unconnected if at the top file level.
Other comment locations will be rejected by the parser.
Maybe accept end of line comments as well. So there would be two slots for comments on each atom or row:
Before Comment and Suffix Comment.

// This is a free standing comment.

// These comment lines will get attached
// To the next set statement as there is not empty line between them.
set foo table (
	// The header should probably allow a comment.
	name | type // This is a suffix comment.

	// You can also write a comment above a data line.
	my_name | int // Or at the end of a data line.
)

However this is not allowed:
set /* invalid comment */ foo table (...)

Multiline comments are allowed, but restricted where they can appear.

Within a table or struct, comments must be attached to a field or row.

### POCv1 Parts
 1. Setup an in-memory database that will accept queries.
 2. Create a table for each type.
 3. Load the parsed statements into the tables.
 4. Analize loaded values in tables.

---


 1. Compile into finished configuration.
 2. Perform a few (not all) checks on configuration.
 3. Turn into a database create script (ignore alter).
 4. Turn into SQL queries.


// 1. Setting properties like this is nice in that it left-aligns things well.
// 2. Setting properties like this is not nice as it seems redundant often.
// 3. It would be useful to be able to pre-declare a schema.
// 4. It would be useful to have some limited key value setting.
// 5. It would be useful to have a table with common columns, then have other columns be key value pairs.


schema database struct(
	db_name key
	table struct(
		t_name key
		column table(
			name | type |
			name | key
			type | type | {required: true}
			length | int
		)
		index table(
			name | type
			name | key
			index | list(
				// Columns or expressions from this table.
				context database.:db_name.table.:t_name
				or(ref , query)
			) | {required: true}
			include | list(
				// Columns or expressions from this table.
				context database.:db_name.table.:t_name
				or(ref , query)
			) | {required: false}
			where query
		)
	)
)
// Top level is always a some type of statement action "set", "schema", "var", "create" ...
// We are making a data object that could be exported as some type of JSON.
// It is important to be able to divide up the declaration.
// PERHAPS: only the package that creates an object can modify it.
// Thus there might be some type of "main" package per database that composes
// various other sub packages to create the final output.
// Perhaps, it would be possible to "execute" a query to help construct tagged columns
// or other normal features of a database.

// WHAT IS THE EXACT SYNTAX?

statement :: statement_type identifier [value]

value :: text | number | bool | - (nil) | struct | list | table | query

struct :: struct([field EOS...])

field :: identifier value

list :: list([context idnetifier EOS] query)

table :: table(...)

// Oh! a table definition should be a different type then a table value.
// And a list definition should be a different type then a list value.
// And a struct definition should be different then a struct value.
// The definition could have the same keyword, but it would have a different meaning under the schema statement.
// This will only work if the lexer is the same, which I have every reason to think it will be.

schema_struct :: struct([schema_field EOS...])

schema_field :: identifier query_match (type) [ query_value (default)]

schema_list :: list(query_match) // Each element must match the query match.

schema_table :: table(
	name | type [ | default ] EOS
	[identifier | query_match [ | query_value ] EOS ...
)

query :: query(
	query_type parts...

	from table_name tn
	from sub_table_name stn eq(stn.parent, tn.id)
	and eq(tn.cono, 1)
	select tn.name, stn.item
)

// This makes sense for the most part. Except for the list and table definitions.

// Once a schema is created, it cannot be modified. BUT, it may be overlayed.
// For instance, perhaps you put in the postgresql overlay, which may add additional database types:
// name | type (general DB type) | pg.type (pg specific type) | ms.type
// Then a viewer may choose an overlay to view the schema from.
//
// In a similar way, a portal may be created by copying the database schema over the portal name
// then filling in the rest of the bits.

schema portal.library portals.create(database.library)

// set portal.library.table.books.columns

// Here is more thinking:

type number property (
	min number
	max number
)
type text property (
	length number
)
type decimal property (
	precision number
	scale number
)

set database.library.table.book table(
	name | type
	id | key
	title | text(100) or text(length 100)
	price | decimal(16;2) or decimal(precision 16; scale 2)
	stock | number(min 0; max 1000000; decimal: 0)
	price2 | number(min 0; max 10000; decimal: 2)
)

// Don't worry about extending the model at this time.
// Yes, we will want to extend the model in the future.
// But there isn't anything dramatic from adding that design.

schema table schema ( 
	name | text(min: 1, max: 1000)
	type | $.type
	nullable | bool | (default false)
)

// For now, I could make the "set" identifier arbitrary.
// What would matter would be the "type" before the "(".
// Then resolving symbols could be done at parse time with
// better logic. This would have some limitations, but it
// would also probably work through the POC release.
// Make all the schema types pre-declared for POCv1.
// If we can generate the sample library application with POCv1,
// then work on moving schema out of pre-declare and introducing
// custom schemas in POCv2.

// If we modify the query based on which parameters are available, then
// it is highly likely we would need to create a server to run queries
// on the database server. But this has its own issues. Let's ignore this
// for now. It could be that we compile to:
//  * database schema
//  * set of stored procedures
//  * a JSON object that configures the front end
//  * a JSON object (or Go code) that configures the back end


---

schema database.* table(
	name | type | key
	name | type | key
)

schema database.*.table.* table(
	name | type | key
	name | type | key
)

schema database.*.table.*.columns.* table(
	name | type | key
	name | type | key
	default | query
	key | bool
	length | number
)

schema database.*.table.*.indices.* table(
	name | type | key
	name | type | key
	index | list(context ../columns)
	include | list(context ../columns)
	where | query
)

---

The difference between XML Schema and SCD is that XML Schema seeks to constrain and validate an XML document.
SCD also has a schema component, but attempts to make it directly writable, and the validation may also apply
to an external system, not just the XML Schema. For instance, SCD query may be executed on the SCD document itself,
or it may be used to transpile the query into a SQL database.

SCD is designed to project definitions out into other systems.
XML Schema is designed to work within XML documents.
While the XML Documents are used in other systems, they doen't work together by default. The data and the schema
are too sharply divided.

In SCD, an int64 may constrain a SCD type or be the type of a SQL Database (bigint) or be an integer in the source UI.
In fact, this is designed to flow into whatever system it uses.

